{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from datetime import date, timedelta\n",
    "import pickle\n",
    "import Model_func as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Flatten, TimeDistributed, LeakyReLU, Conv2D, Concatenate, BatchNormalization, MaxPooling2D, AveragePooling1D, Reshape, Conv1D\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras import layers, optimizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'D:\\data\\train_local\\train_v1', 'rb') as fp:\n",
    "    train = pickle.load(fp)\n",
    "with open(r'D:\\data\\train_local\\val_v1', 'rb') as fp:\n",
    "    val = pickle.load(fp)\n",
    "with open(r'D:\\data\\train_local\\test_v1', 'rb') as fp:\n",
    "    test = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer Data Form to trainable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Separating Asset and Trade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trade = np.array([[data[i].values.tolist() for i in range(2)] for data in train])\n",
    "train_asset = np.array([[data[i].values.tolist() for i in range(2, 5)] for data in train])\n",
    "train_demo = np.array([data[5] for data in train])\n",
    "label_train = [data[7] for data in train]\n",
    "Y_train = pd.DataFrame(label_train, columns=['label'])\n",
    "Y_train = pd.get_dummies(Y_train, columns=['label'])\n",
    "Y_train = Y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_trade = np.array([[data[i].values.tolist() for i in range(2)] for data in val])\n",
    "val_asset = np.array([[data[i].values.tolist() for i in range(2, 5)] for data in val])\n",
    "val_demo = np.array([data[5] for data in val])\n",
    "label_val = [data[7] for data in val]\n",
    "Y_val = pd.DataFrame(label_val, columns=['label'])\n",
    "Y_val = pd.get_dummies(Y_val, columns=['label'])\n",
    "Y_val = Y_val.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_trade = np.array([[data[i].values.tolist() for i in range(2)] for data in test])\n",
    "test_asset = np.array([[data[i].values.tolist() for i in range(2, 5)] for data in test])\n",
    "test_demo = np.array([data[5] for data in test])\n",
    "label_test = [data[7] for data in test]\n",
    "Y_test = pd.DataFrame(label_test, columns=['label'])\n",
    "Y_test = pd.get_dummies(Y_test, columns=['label'])\n",
    "Y_test = Y_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Separating Asset(different asset) and Trade(Buy and sell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_trade_B = np.array([data[0].values.tolist() for data in train])\n",
    "train_trade_S = np.array([data[1].values.tolist() for data in train])\n",
    "train_asset_ST = np.array([data[2].values.tolist() for data in train])\n",
    "train_asset_MP = np.array([data[3].values.tolist() for data in train])\n",
    "train_asset_SS = np.array([data[4].values.tolist() for data in train])\n",
    "train_demo = np.array([data[5] for data in train])\n",
    "label_train = [data[7] for data in train]\n",
    "Y_train = pd.DataFrame(label_train, columns=['label'])\n",
    "Y_train = pd.get_dummies(Y_train, columns=['label'])\n",
    "Y_train = Y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_trade_B = np.array([data[0].values.tolist() for data in val])\n",
    "val_trade_S = np.array([data[1].values.tolist() for data in val])\n",
    "val_asset_ST = np.array([data[2].values.tolist() for data in val])\n",
    "val_asset_MP = np.array([data[3].values.tolist() for data in val])\n",
    "val_asset_SS = np.array([data[4].values.tolist() for data in val])\n",
    "val_demo = np.array([data[5] for data in val])\n",
    "label_val = [data[7] for data in val]\n",
    "Y_val = pd.DataFrame(label_val, columns=['label'])\n",
    "Y_val = pd.get_dummies(Y_val, columns=['label'])\n",
    "Y_val = Y_val.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_trade_B = np.array([data[0].values.tolist() for data in test])\n",
    "test_trade_S = np.array([data[1].values.tolist() for data in test])\n",
    "test_asset_ST = np.array([data[2].values.tolist() for data in test])\n",
    "test_asset_MP = np.array([data[3].values.tolist() for data in test])\n",
    "test_asset_SS = np.array([data[4].values.tolist() for data in test])\n",
    "test_demo = np.array([data[5] for data in test])\n",
    "label_test = [data[7] for data in test]\n",
    "Y_test = pd.DataFrame(label_test, columns=['label'])\n",
    "Y_test = pd.get_dummies(Y_test, columns=['label'])\n",
    "Y_test = Y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 120, 11)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_trade.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_trade = Input(train_trade.shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treat asset and trading data as image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40655 samples, validate on 1967 samples\n",
      "Epoch 1/10\n",
      "40655/40655 - 26s - loss: 0.4439 - auc_9: 0.8751 - val_loss: 0.3930 - val_auc_9: 0.9042\n",
      "Epoch 2/10\n",
      "40655/40655 - 25s - loss: 0.4126 - auc_9: 0.8934 - val_loss: 0.3961 - val_auc_9: 0.9033\n",
      "Epoch 3/10\n",
      "40655/40655 - 25s - loss: 0.4015 - auc_9: 0.8993 - val_loss: 0.3952 - val_auc_9: 0.9030\n",
      "Epoch 4/10\n",
      "40655/40655 - 26s - loss: 0.3944 - auc_9: 0.9030 - val_loss: 0.3795 - val_auc_9: 0.9144\n",
      "Epoch 5/10\n",
      "40655/40655 - 25s - loss: 0.3850 - auc_9: 0.9077 - val_loss: 0.3733 - val_auc_9: 0.9149\n",
      "Epoch 6/10\n",
      "40655/40655 - 25s - loss: 0.3760 - auc_9: 0.9123 - val_loss: 0.3703 - val_auc_9: 0.9164\n",
      "Epoch 7/10\n",
      "40655/40655 - 26s - loss: 0.3713 - auc_9: 0.9145 - val_loss: 0.3776 - val_auc_9: 0.9112\n",
      "Epoch 8/10\n",
      "40655/40655 - 26s - loss: 0.3582 - auc_9: 0.9207 - val_loss: 0.4028 - val_auc_9: 0.9016\n",
      "Epoch 9/10\n",
      "40655/40655 - 26s - loss: 0.3485 - auc_9: 0.9251 - val_loss: 0.3858 - val_auc_9: 0.9072\n",
      "Epoch 10/10\n",
      "40655/40655 - 26s - loss: 0.3375 - auc_9: 0.9300 - val_loss: 0.3892 - val_auc_9: 0.9060\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x11c5538f0f0>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_trade = Input(train_trade.shape[1:])\n",
    "trade_model = Conv2D(filters=5, \n",
    "              strides=1, \n",
    "              padding='valid', \n",
    "              activation='relu',\n",
    "              kernel_size=(2, 2),\n",
    "              data_format='channels_first')(input_trade)\n",
    "trade_model = Dropout(0.2)(trade_model)\n",
    "trade_model = Conv2D(filters=1, \n",
    "              strides=1, \n",
    "              padding='valid', \n",
    "              activation='relu',\n",
    "              kernel_size=(5, 1),\n",
    "              data_format='channels_first')(trade_model)\n",
    "trade_model = Dropout(0.2)(trade_model)\n",
    "trade_model = Flatten()(trade_model)\n",
    "input_asset = Input(train_asset.shape[1:])\n",
    "asset_model = Conv2D(filters=3, \n",
    "              strides=1, \n",
    "              padding='valid', \n",
    "              activation='relu',\n",
    "              kernel_size=(5, 5),\n",
    "              data_format='channels_first')(input_asset)\n",
    "asset_model = Dropout(0.2)(asset_model)\n",
    "asset_model = Conv2D(filters=1, \n",
    "              strides=1, \n",
    "              padding='valid', \n",
    "              activation='relu',\n",
    "              kernel_size=(5, 1),\n",
    "              data_format='channels_first')(asset_model)\n",
    "asset_model = Dropout(0.2)(asset_model)\n",
    "asset_model = Flatten()(asset_model)\n",
    "input_demographic = Input(train_demo.shape[1:])\n",
    "combinedInput = concatenate([trade_model, asset_model, input_demographic])\n",
    "full_model = Dense(128, activation='relu')(combinedInput)\n",
    "full_model = Dense(32, activation='relu')(full_model)\n",
    "full_model = Dense(8, activation='relu')(full_model)\n",
    "full_model = Dense(2, activation='softmax')(full_model)\n",
    "model = Model([input_trade, input_asset, input_demographic], full_model)\n",
    "opt = optimizers.Nadam(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[tf.keras.metrics.AUC()])\n",
    "model.fit([train_trade, train_asset, train_demo], Y_train, validation_data=([val_trade, val_asset, val_demo], Y_val), batch_size=8, epochs=10, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treat trading and asset data as documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_trade_B = Input(train_trade_B.shape[1:])\n",
    "input_trade_S = Input(train_trade_S.shape[1:])\n",
    "trade_model_B = Conv1D(filters=3, \n",
    "              strides=1, \n",
    "              padding='valid', \n",
    "              activation='relu',\n",
    "              kernel_size=5)(input_trade_B)\n",
    "trade_model_B = Flatten()(trade_model_B)\n",
    "trade_model_S = Conv1D(filters=3, \n",
    "              strides=1, \n",
    "              padding='valid', \n",
    "              activation='relu',\n",
    "              kernel_size=5)(input_trade_S)\n",
    "trade_model_S = Flatten()(trade_model_S)\n",
    "trade_model = concatenate([trade_model_B, trade_model_S])\n",
    "trade_model = Dense(128, activation='relu')(trade_model)\n",
    "asset_model_ST = Conv1D(filters=1, \n",
    "              strides=1, \n",
    "              padding='valid', \n",
    "              activation='relu',\n",
    "              kernel_size=5)(input_asset_ST)\n",
    "asset_model_ST = Flatten()(asset_model_ST)\n",
    "asset_model_MP = Conv1D(filters=1, \n",
    "              strides=1, \n",
    "              padding='valid', \n",
    "              activation='relu',\n",
    "              kernel_size=5)(input_asset_MP)\n",
    "asset_model_MP = Flatten()(asset_model_MP)\n",
    "asset_model_SS = Conv1D(filters=1, \n",
    "              strides=1, \n",
    "              padding='valid', \n",
    "              activation='relu',\n",
    "              kernel_size=5)(input_asset_SS)\n",
    "asset_model_SS = Flatten()(asset_model_SS)\n",
    "trade_model = concatenate([trade_model_B, trade_model_S])\n",
    "asset_model = concatenate([asset_model_ST, asset_model_MP, asset_model_SS])\n",
    "combinedInput = concatenate([trade_model, asset_model])\n",
    "full_model = Dense(128, activation='relu')(combinedInput)\n",
    "full_model = Dense(32, activation='relu')(full_model)\n",
    "full_model = Dense(8, activation='relu')(full_model)\n",
    "full_model = Dense(2, activation='softmax')(full_model)\n",
    "model = Model([input_trade_B, input_trade_S, input_asset_ST, input_asset_MP, input_asset_SS], trade_model)\n",
    "opt = optimizers.Nadam(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_41 (InputLayer)           [(None, 120, 11)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_42 (InputLayer)           [(None, 120, 11)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, 116, 3)       168         input_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, 116, 3)       168         input_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_33 (Flatten)            (None, 348)          0           conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_34 (Flatten)            (None, 348)          0           conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 696)          0           flatten_33[0][0]                 \n",
      "                                                                 flatten_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 128)          89216       concatenate_18[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 89,552\n",
      "Trainable params: 89,552\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40655 samples, validate on 1967 samples\n",
      "Epoch 1/5\n",
      "40655/40655 - 29s - loss: 0.4261 - auc_10: 0.8858 - val_loss: 0.3792 - val_auc_10: 0.9111\n",
      "Epoch 2/5\n",
      "40655/40655 - 28s - loss: 0.3695 - auc_10: 0.9153 - val_loss: 0.3463 - val_auc_10: 0.9258\n",
      "Epoch 3/5\n",
      "40655/40655 - 28s - loss: 0.3451 - auc_10: 0.9264 - val_loss: 0.3411 - val_auc_10: 0.9299\n",
      "Epoch 4/5\n",
      "40655/40655 - 29s - loss: 0.3330 - auc_10: 0.9315 - val_loss: 0.3514 - val_auc_10: 0.9263\n",
      "Epoch 5/5\n",
      "40655/40655 - 28s - loss: 0.3219 - auc_10: 0.9359 - val_loss: 0.3427 - val_auc_10: 0.9286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x11c55b4a358>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_trade_B = Input(train_trade_B.shape[1:])\n",
    "input_trade_S = Input(train_trade_S.shape[1:])\n",
    "trade_model_B = Conv1D(filters=3, \n",
    "              strides=1, \n",
    "              padding='valid', \n",
    "              activation='relu',\n",
    "              kernel_size=5)(input_trade_B)\n",
    "trade_model_B = Flatten()(trade_model_B)\n",
    "trade_model_S = Conv1D(filters=3, \n",
    "              strides=1, \n",
    "              padding='valid', \n",
    "              activation='relu',\n",
    "              kernel_size=5)(input_trade_S)\n",
    "trade_model_S = Flatten()(trade_model_S)\n",
    "trade_model = Concatenate(axis=-1)([trade_model_B, trade_model_S])\n",
    "input_asset_ST = Input(train_asset_ST.shape[1:])\n",
    "input_asset_MP = Input(train_asset_MP.shape[1:])\n",
    "input_asset_SS = Input(train_asset_SS.shape[1:])\n",
    "asset_model_ST = Conv1D(filters=3, \n",
    "              strides=1, \n",
    "              padding='valid', \n",
    "              activation='relu',\n",
    "              kernel_size=5)(input_asset_ST)\n",
    "asset_model_ST = Flatten()(asset_model_ST)\n",
    "asset_model_MP = Conv1D(filters=3, \n",
    "              strides=1, \n",
    "              padding='valid', \n",
    "              activation='relu',\n",
    "              kernel_size=5)(input_asset_MP)\n",
    "asset_model_MP = Flatten()(asset_model_MP)\n",
    "asset_model_SS = Conv1D(filters=3, \n",
    "              strides=1, \n",
    "              padding='valid', \n",
    "              activation='relu',\n",
    "              kernel_size=5)(input_asset_SS)\n",
    "asset_model_SS = Flatten()(asset_model_SS)\n",
    "asset_model = Concatenate(axis=-1)([asset_model_ST, asset_model_MP, asset_model_SS])\n",
    "input_demographic = Input(train_demo.shape[1:])\n",
    "combinedInput = Concatenate()([trade_model, asset_model, input_demographic])\n",
    "full_model = Dense(128, activation='relu')(combinedInput)\n",
    "full_model = Dense(32, activation='relu')(full_model)\n",
    "full_model = Dense(8, activation='relu')(full_model)\n",
    "full_model = Dense(2, activation='softmax')(full_model)\n",
    "model = Model([input_trade_B, input_trade_S, input_asset_ST, input_asset_MP, input_asset_SS, input_demographic], full_model)\n",
    "opt = optimizers.Nadam(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[tf.keras.metrics.AUC()])\n",
    "model.fit([train_trade_B, train_trade_S, train_asset_ST, train_asset_MP, train_asset_SS, train_demo], Y_train, validation_data=([val_trade_B, val_trade_S, val_asset_ST, val_asset_MP, val_asset_SS, val_demo], Y_val), batch_size=8, epochs=5, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8572778827977315\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      ACTIVE       0.90      0.80      0.84       514\n",
      "       CHURN       0.83      0.91      0.87       544\n",
      "\n",
      "    accuracy                           0.86      1058\n",
      "   macro avg       0.86      0.86      0.86      1058\n",
      "weighted avg       0.86      0.86      0.86      1058\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = func.Evaluation([test_trade_B, test_trade_S, test_asset_ST, test_asset_MP, test_asset_SS, test_demo], label_test, threshold=0.5, model=model)\n",
    "print(result[0])\n",
    "print(result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8393194706994329\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      ACTIVE       0.85      0.82      0.83       514\n",
      "       CHURN       0.83      0.86      0.85       544\n",
      "\n",
      "    accuracy                           0.84      1058\n",
      "   macro avg       0.84      0.84      0.84      1058\n",
      "weighted avg       0.84      0.84      0.84      1058\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = func.Evaluation([test_trade, test_asset, test_demo], label_test, threshold=0.5, model=model)\n",
    "print(result[0])\n",
    "print(result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(r'D:\\Customer_Value\\model\\first_stage_1D.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation on other samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0303 09:03:22.524970 12480 deprecation.py:506] From C:\\Users\\011553\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0303 09:03:22.527948 12480 deprecation.py:506] From C:\\Users\\011553\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0303 09:03:22.528949 12480 deprecation.py:506] From C:\\Users\\011553\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = load_model(r'D:\\Customer_Value\\model\\first_stage.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation on another sample with 100% active data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'D:\\data\\evaluate\\local\\evaluate_large', 'rb') as fp:\n",
    "    eval_test = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_trade_B = np.array([data[0].values.tolist() for data in eval_test])\n",
    "# eval_trade_S = np.array([data[1].values.tolist() for data in eval_test])\n",
    "# eval_asset_ST = np.array([data[2].values.tolist() for data in eval_test])\n",
    "# eval_asset_MP = np.array([data[3].values.tolist() for data in eval_test])\n",
    "# eval_asset_SS = np.array([data[4].values.tolist() for data in eval_test])\n",
    "eval_demo = np.array([data[5] for data in eval_test])\n",
    "label_test_eval = [data[7] for data in eval_test]\n",
    "Y_test_eval = pd.DataFrame(label_test_eval, columns=['label'])\n",
    "Y_test_eval = pd.get_dummies(Y_test_eval, columns=['label'])\n",
    "Y_test_eval = Y_test_eval.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_test_trade = np.array([[data[i].values.tolist() for i in range(2)] for data in eval_test])\n",
    "# eval_test_asset = np.array([[data[i].values.tolist() for i in range(2, 5)] for data in eval_test])\n",
    "eval_test_demo = np.array([data[5] for data in eval_test])\n",
    "label_test_eval = [data[7] for data in eval_test]\n",
    "Y_test_eval = pd.DataFrame(label_test_eval, columns=['label'])\n",
    "Y_test_eval = pd.get_dummies(Y_test_eval, columns=['label'])\n",
    "Y_test_eval = Y_test_eval.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1041, 2, 120, 11)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_trade.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\011553\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83064\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      ACTIVE       1.00      0.83      0.91    150000\n",
      "       CHURN       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.83    150000\n",
      "   macro avg       0.50      0.42      0.45    150000\n",
      "weighted avg       1.00      0.83      0.91    150000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = func.Evaluation([eval_test_trade, eval_test_asset, eval_test_demo], label_test_eval, model, threshold=0.5)\n",
    "print(result[0])\n",
    "print(result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\011553\\AppData\\Local\\Continuum\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\metrics\\classification.py:1439: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7871333333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      ACTIVE       1.00      0.79      0.88    150000\n",
      "       CHURN       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.79    150000\n",
      "   macro avg       0.50      0.39      0.44    150000\n",
      "weighted avg       1.00      0.79      0.88    150000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = func.Evaluation([eval_trade_B, eval_trade_S, eval_asset_ST, eval_asset_MP, eval_asset_SS, eval_demo], label_test_eval, threshold=0.5, model=model)\n",
    "print(result[0])\n",
    "print(result[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-gpu)",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
